{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564b2529-08e5-447a-a05c-c435dd4098a5",
   "metadata": {},
   "source": [
    "# Unsupervised Anomaly Detection on Tennessee Eastman Process (TEP) Data using Transformer Autoencoder and Kalman Filter\n",
    "\n",
    "**Objective:** This notebook implements an unsupervised anomaly detection system for the Tennessee Eastman Process (TEP) dataset. The system utilizes a Transformer-based autoencoder to learn normal operational behavior and a Kalman filter to smooth anomaly scores for more robust detection.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Data Loading and Preparation:**\n",
    "    * Load fault-free training, fault-free testing, and faulty testing datasets for the TEP.\n",
    "    * Identify unique fault types present in the faulty testing data.\n",
    "2.  **Unsupervised Model Training:**\n",
    "    * **Preprocessing:** Apply StandardScaler to the features.\n",
    "    * **Time Series Windowing:** Convert the time series data into overlapping windows to serve as input sequences for the Transformer model.\n",
    "    * **Model Architecture:**\n",
    "        * **Transformer Autoencoder:** An autoencoder with Transformer encoder layers is used. It learns to reconstruct normal input sequences. Anomalies are expected to have higher reconstruction errors.\n",
    "        * **Positional Encoding:** Added to the input embeddings to provide the model with information about the position of data points within a sequence.\n",
    "    * **Training Loop:**\n",
    "        * Train the autoencoder on fault-free training data to minimize reconstruction loss (Mean Squared Error).\n",
    "        * Validate the model on a hold-out set of fault-free data.\n",
    "        * Save model checkpoints periodically and the best performing model based on validation loss.\n",
    "    * **Threshold Setting:** Determine an anomaly threshold based on the reconstruction errors from the fault-free validation data (e.g., 95th percentile).\n",
    "    * **Kalman Filter:** A Kalman filter is initialized to smooth the raw reconstruction error scores, making anomaly detection less susceptible to noise.\n",
    "3.  **Model Evaluation:**\n",
    "    * Load the trained model and scaler.\n",
    "    * If the threshold wasn't set or loaded properly, calculate it using fault-free test data.\n",
    "    * For each specified fault type (including normal operation - fault type 0):\n",
    "        * Prepare test data (scaling and windowing).\n",
    "        * Generate predictions using the trained autoencoder and the determined threshold.\n",
    "        * Apply the Kalman filter to the raw anomaly scores.\n",
    "        * Calculate various performance metrics: Precision, Recall, F1-Score, Accuracy, ROC-AUC, PR-AUC, and components of the confusion matrix (TP, FP, TN, FN).\n",
    "        * Generate and save visualizations:\n",
    "            * Detailed evaluation plots (raw/filtered scores, true vs. predicted labels, score distributions).\n",
    "            * Confusion matrix heatmaps.\n",
    "            * ROC and Precision-Recall curves (if applicable).\n",
    "    * Generate and save a summary report (CSV) and summary visualizations (F1 scores bar chart, metrics heatmap, confusion components bar charts) across all evaluated fault types.\n",
    "\n",
    "**Libraries Used:**\n",
    "* `matplotlib` for plotting.\n",
    "* `numpy` for numerical operations.\n",
    "* `pandas` for data manipulation.\n",
    "* `pyreadr` for loading RData files.\n",
    "* `torch` (PyTorch) for building and training the neural network.\n",
    "* `gc` for garbage collection.\n",
    "* `sklearn` for preprocessing (StandardScaler) and metrics.\n",
    "* `tqdm` for progress bars.\n",
    "* `os` for interacting with the file system.\n",
    "* `time` for timing operations.\n",
    "* `pickle` for saving/loading Python objects (like the scaler).\n",
    "* `seaborn` for enhanced visualizations.\n",
    "* `warnings` to manage warning messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7305fb0-af70-46c3-b7cd-6e79f72fd992",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2fdf90-06b0-4fbb-8c32-b620463a004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Available fault types: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "================================================================================\n",
      "Training unsupervised anomaly detector\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at checkpoints\\unsupervised_checkpoint.pth, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.0567, Val Loss: 0.0046, Time: 320.71s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0046 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 0.0006, Val Loss: 0.0003, Time: 317.41s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0003 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.0002, Val Loss: 0.0001, Time: 317.62s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0001 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.0002, Val Loss: 0.0004, Time: 316.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.0002, Val Loss: 0.0001, Time: 314.39s\n",
      "Checkpoint saved at checkpoints\\unsupervised_checkpoint_epoch_5.pth\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0001 at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.0002, Val Loss: 0.0002, Time: 316.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 315.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 0.0001, Val Loss: 0.0002, Time: 316.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 314.28s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0000 at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 315.72s\n",
      "Checkpoint saved at checkpoints\\unsupervised_checkpoint_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 318.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 315.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 316.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 317.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 0.0001, Val Loss: 0.0004, Time: 315.81s\n",
      "Checkpoint saved at checkpoints\\unsupervised_checkpoint_epoch_15.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Train Loss: 0.0001, Val Loss: 0.0002, Time: 316.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 316.68s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0000 at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 317.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 317.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 316.78s\n",
      "Checkpoint saved at checkpoints\\unsupervised_checkpoint_epoch_20.pth\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0000 at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 314.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 323.90s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0000 at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Train Loss: 0.0001, Val Loss: 0.0002, Time: 329.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 326.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Train Loss: 0.0001, Val Loss: 0.0001, Time: 324.56s\n",
      "Checkpoint saved at checkpoints\\unsupervised_checkpoint_epoch_25.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 323.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Train Loss: 0.0001, Val Loss: 0.0003, Time: 325.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Train Loss: 0.0000, Val Loss: 0.0000, Time: 326.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Train Loss: 0.0001, Val Loss: 0.0000, Time: 326.38s\n",
      "Checkpoint saved at checkpoints\\unsupervised_best_model.pth\n",
      "New best model saved with loss: 0.0000 at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Train Loss: 0.0000, Val Loss: 0.0000, Time: 326.64s\n",
      "Checkpoint saved at checkpoints\\unsupervised_checkpoint_epoch_30.pth\n",
      "Checkpoint saved at checkpoints\\unsupervised_final_model.pth\n",
      "Training complete! Final model saved after epoch 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing threshold: 100%|████████████████████████████████████████████████████████| 1562/1562 [00:25<00:00, 60.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.0000 based on 95th percentile of validation errors.\n",
      "Checkpoint saved at checkpoints\\unsupervised_final_model_with_threshold.pth\n",
      "StandardScaler saved to scaler.pkl\n",
      "\n",
      "================================================================================\n",
      "Unsupervised model training run complete.\n",
      "Model has been saved in the checkpoints directory.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set device for PyTorch (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load datasets\n",
    "df_train_faultfree = pyreadr.read_r(\"TEP_FaultFree_Training.RData\")['fault_free_training']\n",
    "df_test_faultfree = pyreadr.read_r(\"TEP_FaultFree_Testing.RData\")['fault_free_testing']\n",
    "df_test_faulty = pyreadr.read_r(\"TEP_Faulty_Testing.RData\")['faulty_testing']\n",
    "\n",
    "# Get all fault types\n",
    "fault_types = sorted(df_test_faulty['faultNumber'].unique())\n",
    "print(f\"Available fault types: {fault_types}\")\n",
    "\n",
    "# Kalman Filter implementation for smoothing predictions\n",
    "class KalmanFilter:\n",
    "    \"\"\"\n",
    "    Simple implementation of Kalman filter for 1D signal smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_x, Q=1e-4, R=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Kalman filter\n",
    "\n",
    "        Args:\n",
    "            dim_x: State dimension\n",
    "            Q: Process noise covariance\n",
    "            R: Measurement noise covariance\n",
    "        \"\"\"\n",
    "        self.dim_x = dim_x\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "\n",
    "    def filter(self, y_noisy):\n",
    "        \"\"\"\n",
    "        Apply Kalman filtering to noisy observations\n",
    "\n",
    "        Args:\n",
    "            y_noisy: Noisy observations\n",
    "\n",
    "        Returns:\n",
    "            x_est: Filtered (smoothed) state estimates\n",
    "        \"\"\"\n",
    "        n = len(y_noisy)\n",
    "        x_est = np.zeros(n)  # State estimates\n",
    "        P = np.zeros(n)      # Error covariance\n",
    "\n",
    "        # Initial state and covariance\n",
    "        if n > 0:\n",
    "            x_est[0] = y_noisy[0]\n",
    "            P[0] = 1.0\n",
    "        else:\n",
    "            return x_est # Return empty if no data\n",
    "\n",
    "        for k in range(1, n):\n",
    "            # Prediction step\n",
    "            x_pred = x_est[k-1]      # State prediction\n",
    "            P_pred = P[k-1] + self.Q  # Covariance prediction\n",
    "\n",
    "            # Update step with measurement\n",
    "            K = P_pred / (P_pred + self.R)  # Kalman gain\n",
    "            x_est[k] = x_pred + K * (y_noisy[k] - x_pred)  # State update\n",
    "            P[k] = (1 - K) * P_pred    # Covariance update\n",
    "\n",
    "        return x_est\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class UnsupervisedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for unsupervised learning (no labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "\n",
    "# Create sliding window time series input\n",
    "def create_time_series_windows(X, lookback=15, max_samples=None, start_idx=0):\n",
    "    \"\"\"\n",
    "    Create sliding window dataset for unsupervised learning\n",
    "    with memory efficiency optimizations and proper handling of time order\n",
    "    \n",
    "    This modified version ensures no data leakage across time boundaries\n",
    "    \n",
    "    Args:\n",
    "        X: Features array\n",
    "        lookback: Number of time steps to look back\n",
    "        max_samples: Maximum number of samples to use (for memory constrained environments)\n",
    "        start_idx: Start index to avoid overlapping with other datasets (prevents data leakage)\n",
    "        \n",
    "    Returns:\n",
    "        X_windows: Tensor of shape (n_samples, lookback, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate valid range for window creation (respecting time boundaries)\n",
    "    # Number of windows that can be created.\n",
    "    n_potential_samples = len(X) - lookback + 1 - start_idx\n",
    "    # Ensure n_potential_samples is not negative\n",
    "    n_samples = max(0, n_potential_samples)\n",
    "\n",
    "\n",
    "    # Handle max_samples limit if specified\n",
    "    if max_samples is not None and n_samples > max_samples:\n",
    "        # Take consecutive samples in time order (NOT random sampling)\n",
    "        n_samples = max_samples\n",
    "    \n",
    "    # Pre-allocate memory for efficiency\n",
    "    n_features = X.shape[1]\n",
    "    X_windows = np.zeros((n_samples, lookback, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Process in batches to save memory ( tqdm original had range(0, n_samples, batch_size) )\n",
    "    # Corrected loop for creating windows: iterate up to n_samples\n",
    "    for i in tqdm(range(n_samples), desc=\"Creating windows\", miniters=int(n_samples/100) if n_samples > 100 else 1, leave=False):\n",
    "        actual_idx = start_idx + i\n",
    "        X_windows[i] = X[actual_idx : actual_idx + lookback]\n",
    "    \n",
    "    return torch.tensor(X_windows, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Positional Encoding for Transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for Transformer model\n",
    "    Adds information about position of tokens in the sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Transformer-based autoencoder for unsupervised anomaly detection\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based autoencoder for unsupervised anomaly detection\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Transformer autoencoder\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            d_model: Hidden dimension for the model\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input embedding\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=256, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Decoder to reconstruct the input\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            x_reconstructed: Reconstructed input\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Embedding and scale\n",
    "        x_embedded = self.embedding(x) * np.sqrt(self.embedding.out_features)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x_embedded = self.pos_encoder(x_embedded)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x_encoded = self.transformer_encoder(x_embedded)\n",
    "        \n",
    "        # Decode each time step\n",
    "        x_reconstructed = torch.zeros_like(x)\n",
    "        for i in range(seq_len):\n",
    "            x_reconstructed[:, i] = self.decoder(x_encoded[:, i])\n",
    "            \n",
    "        return x_reconstructed\n",
    "    \n",
    "    def get_reconstruction_error(self, x):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error (MSE) for anomaly detection\n",
    "        with memory-efficient implementation\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            error: Reconstruction error\n",
    "        \"\"\"\n",
    "        # Process one sample at a time if batch size is 1\n",
    "        if x.size(0) == 1:\n",
    "            x_reconstructed = self(x)\n",
    "            error = torch.mean(torch.pow(x - x_reconstructed, 2), dim=(1, 2))\n",
    "            return error\n",
    "        \n",
    "        # For larger batches, split into smaller sub-batches if needed\n",
    "        batch_size = x.size(0)\n",
    "        if batch_size > 4:  # If batch is large\n",
    "            errors = []\n",
    "            sub_batch_size = 4  # Process 4 samples at a time\n",
    "            \n",
    "            for i in range(0, batch_size, sub_batch_size):\n",
    "                end_idx = min(i + sub_batch_size, batch_size)\n",
    "                sub_batch = x[i:end_idx]\n",
    "                sub_reconstructed = self(sub_batch)\n",
    "                sub_error = torch.mean(torch.pow(sub_batch - sub_reconstructed, 2), dim=(1, 2))\n",
    "                errors.append(sub_error)\n",
    "                \n",
    "                # Free memory\n",
    "                del sub_batch, sub_reconstructed\n",
    "                \n",
    "            return torch.cat(errors)\n",
    "        else:\n",
    "            # Standard processing for reasonable batch sizes\n",
    "            x_reconstructed = self(x)\n",
    "            error = torch.mean(torch.pow(x - x_reconstructed, 2), dim=(1, 2))\n",
    "            return error\n",
    "\n",
    "\n",
    "# Neural network anomaly detector with reconstruction-based approach\n",
    "class UnsupervisedAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Unsupervised anomaly detector based on reconstruction error\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1, Q=1e-4, R=0.1):\n",
    "        \"\"\"\n",
    "        Initialize detector\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            d_model: Hidden dimension for transformer\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of transformer layers\n",
    "            dropout: Dropout probability\n",
    "            Q: Process noise for Kalman filter\n",
    "            R: Measurement noise for Kalman filter\n",
    "        \"\"\"\n",
    "        self.autoencoder = TransformerAutoencoder(input_dim, d_model, nhead, num_layers, dropout)\n",
    "        self.kalman_filter = KalmanFilter(1, Q, R)\n",
    "        self.device = device\n",
    "        self.autoencoder.to(self.device)\n",
    "        \n",
    "        # Training related variables\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = []\n",
    "        self.current_epoch = 0 # Stores the last completed epoch (0-indexed)\n",
    "        \n",
    "        # Threshold for anomaly detection\n",
    "        self.threshold = None\n",
    "        \n",
    "        # Checkpoint directory\n",
    "        self.checkpoint_dir = 'checkpoints'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, epoch, optimizer, loss, filename=None):\n",
    "        \"\"\"\n",
    "        Save model checkpoint - safely handles None optimizer\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch (1-based for saving convention)\n",
    "            optimizer: Optimizer state (can be None for inference-only checkpoints)\n",
    "            loss: Current loss value\n",
    "            filename: Name of the checkpoint file\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            filename = f'unsupervised_checkpoint.pth'\n",
    "        path = os.path.join(self.checkpoint_dir, filename)\n",
    "    \n",
    "        # Create checkpoint dictionary\n",
    "        checkpoint_dict = {\n",
    "            'epoch': epoch, # This is the 1-based epoch number\n",
    "            'model_state_dict': self.autoencoder.state_dict(),\n",
    "            'loss': loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'epochs': self.epochs, # List of 1-based epoch numbers trained\n",
    "            'current_epoch': self.current_epoch, # Last completed 0-indexed epoch\n",
    "            'threshold': self.threshold\n",
    "        }\n",
    "    \n",
    "        # Only add optimizer state if it's not None\n",
    "        if optimizer is not None:\n",
    "            try:\n",
    "                checkpoint_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "            except AttributeError:\n",
    "                print(\"Warning: optimizer does not have state_dict method, not saving optimizer state\")\n",
    "    \n",
    "        torch.save(checkpoint_dict, path)\n",
    "        print(f\"Checkpoint saved at {path}\")\n",
    "        \n",
    "    def load_checkpoint(self, optimizer, filename=None):\n",
    "        \"\"\"\n",
    "        Load model checkpoint\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer to load state into (can be None)\n",
    "            filename: Name of the checkpoint file\n",
    "            \n",
    "        Returns:\n",
    "            start_epoch: Epoch to start training from (0-indexed for loop)\n",
    "            loss: Loss value of the checkpoint\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            filename = f'unsupervised_checkpoint.pth'\n",
    "        path = os.path.join(self.checkpoint_dir, filename)\n",
    "        if os.path.exists(path):\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                \n",
    "            self.train_losses = checkpoint.get('train_losses', [])\n",
    "            self.val_losses = checkpoint.get('val_losses', [])\n",
    "            self.epochs = checkpoint.get('epochs', [])\n",
    "            self.current_epoch = checkpoint.get('current_epoch', 0) # Last completed 0-indexed epoch\n",
    "            self.threshold = checkpoint.get('threshold', None)\n",
    "            \n",
    "            # 'epoch' in checkpoint is 1-based, so start_epoch for training loop is this value\n",
    "            # current_epoch (0-indexed) is more reliable for determining next epoch.\n",
    "            start_epoch_for_loop = self.current_epoch + 1 if self.epochs else 0\n",
    "\n",
    "\n",
    "            print(f\"Checkpoint loaded from {path}, resuming from epoch {start_epoch_for_loop}\")\n",
    "            return start_epoch_for_loop, checkpoint.get('loss', float('inf'))\n",
    "        else:\n",
    "            print(f\"No checkpoint found at {path}, starting from scratch\")\n",
    "            return 0, float('inf') # Start from epoch 0 (0-indexed)\n",
    "        \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"\n",
    "        Validate model on validation set\n",
    "        \n",
    "        Args:\n",
    "            val_loader: Validation data loader\n",
    "            \n",
    "        Returns:\n",
    "            avg_val_loss: Average validation loss\n",
    "        \"\"\"\n",
    "        self.autoencoder.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch in val_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                X_reconstructed = self.autoencoder(X_batch)\n",
    "                \n",
    "                # MSE reconstruction loss\n",
    "                loss = torch.mean(torch.pow(X_batch - X_reconstructed, 2))\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = total_loss / len(val_loader) if len(val_loader) > 0 else 0.0\n",
    "        return avg_val_loss\n",
    "        \n",
    "    def train_model(self, train_loader, val_loader=None, num_epochs=30, lr=1e-3, resume=True, save_interval=5):\n",
    "        \"\"\"\n",
    "        Train the model with checkpoint support and real-time plotting\n",
    "        \n",
    "        Args:\n",
    "            train_loader: Training data loader (normal samples only)\n",
    "            val_loader: Validation data loader\n",
    "            num_epochs: Number of epochs to train in this session\n",
    "            lr: Learning rate\n",
    "            resume: Whether to resume from checkpoint\n",
    "            save_interval: Interval to save checkpoints\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.autoencoder.parameters(), lr=lr)\n",
    "        \n",
    "        start_epoch_idx = 0 # This will be the 0-indexed epoch to start the loop from\n",
    "        best_loss = float('inf')\n",
    "        if resume:\n",
    "            start_epoch_idx, best_loss = self.load_checkpoint(optimizer)\n",
    "            # self.current_epoch is already updated by load_checkpoint to the last *completed* 0-indexed epoch\n",
    "        \n",
    "        if start_epoch_idx == 0 and not self.epochs: # if not resuming or no history\n",
    "            self.train_losses = []\n",
    "            self.val_losses = []\n",
    "            self.epochs = []\n",
    "            self.current_epoch = -1 # Indicates no epochs completed yet\n",
    "        \n",
    "        self.autoencoder.train()\n",
    "        training_interrupted = False\n",
    "        # The loop runs for `num_epochs` iterations.\n",
    "        # `epoch_iter` goes from 0 to `num_epochs - 1`.\n",
    "        # `current_training_epoch` is the actual epoch number (0-indexed globally).\n",
    "        for epoch_iter in range(num_epochs):\n",
    "            current_training_epoch = start_epoch_idx + epoch_iter # Current 0-indexed epoch number\n",
    "            self.current_epoch = current_training_epoch\n",
    "            epoch_start_time = time.time()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            batch_pbar = tqdm(train_loader, desc=f\"Epoch {current_training_epoch+1}/{start_epoch_idx + num_epochs}\", leave=False)\n",
    "            \n",
    "            try:\n",
    "                for X_batch in batch_pbar:\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    X_reconstructed = self.autoencoder(X_batch)\n",
    "                    loss = torch.mean(torch.pow(X_batch - X_reconstructed, 2))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    batch_pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "                avg_train_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "                self.train_losses.append(avg_train_loss)\n",
    "                self.epochs.append(current_training_epoch + 1) # Store 1-based epoch number\n",
    "                \n",
    "                avg_val_loss = None\n",
    "                if val_loader:\n",
    "                    avg_val_loss = self.validate(val_loader)\n",
    "                    self.val_losses.append(avg_val_loss)\n",
    "                \n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                status = f\"Epoch {current_training_epoch+1}/{start_epoch_idx + num_epochs}, Train Loss: {avg_train_loss:.4f}\"\n",
    "                if avg_val_loss is not None:\n",
    "                    status += f\", Val Loss: {avg_val_loss:.4f}\"\n",
    "                status += f\", Time: {epoch_time:.2f}s\"\n",
    "                print(status)\n",
    "                \n",
    "                if (current_training_epoch + 1) % 5 == 0: # Plot every 5 epochs\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    # Ensure self.epochs is used as x-axis for both train and val losses\n",
    "                    # and that val_losses are aligned with epochs if resuming\n",
    "                    \n",
    "                    # Align train_losses with self.epochs\n",
    "                    plot_epochs = self.epochs\n",
    "                    plot_train_losses = self.train_losses[-len(plot_epochs):] # ensure alignment if lists got desynced\n",
    "                                                                            # (should not happen with current logic)\n",
    "\n",
    "                    plt.plot(plot_epochs, plot_train_losses, 'b-', label='Training Loss')\n",
    "\n",
    "                    if self.val_losses:\n",
    "                        # Determine the correct x-values for validation losses based on when they were recorded\n",
    "                        # Assuming one val_loss per epoch stored in self.epochs\n",
    "                        if len(self.val_losses) <= len(plot_epochs):\n",
    "                             # If val_losses is shorter or equal, align from the end\n",
    "                            val_epochs_x = plot_epochs[-len(self.val_losses):]\n",
    "                            plot_val_losses = self.val_losses\n",
    "                        else: # Should not happen if val_loss appended each epoch\n",
    "                            val_epochs_x = plot_epochs \n",
    "                            plot_val_losses = self.val_losses[-len(val_epochs_x):]\n",
    "\n",
    "                        if len(val_epochs_x) == len(plot_val_losses) and len(val_epochs_x) > 0 :\n",
    "                             plt.plot(val_epochs_x, plot_val_losses, 'r-', label='Validation Loss')\n",
    "\n",
    "\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.title('Reconstruction Loss')\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    plt.savefig(f'unsupervised_loss_epoch_{current_training_epoch+1}.png')\n",
    "                    plt.close()\n",
    "                \n",
    "                if (current_training_epoch + 1) % save_interval == 0:\n",
    "                    self.save_checkpoint(current_training_epoch + 1, optimizer, avg_train_loss, f'unsupervised_checkpoint_epoch_{current_training_epoch+1}.pth')\n",
    "                \n",
    "                current_eval_loss = avg_val_loss if avg_val_loss is not None else avg_train_loss\n",
    "                if current_eval_loss < best_loss:\n",
    "                    best_loss = current_eval_loss\n",
    "                    self.save_checkpoint(current_training_epoch + 1, optimizer, best_loss, f'unsupervised_best_model.pth')\n",
    "                    print(f\"New best model saved with loss: {best_loss:.4f} at epoch {current_training_epoch+1}\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"\\nTraining interrupted by user at epoch {current_training_epoch+1}! Saving checkpoint...\")\n",
    "                training_interrupted = True\n",
    "                # Calculate loss for completed batches in the interrupted epoch\n",
    "                num_batches_done = len(batch_pbar) - batch_pbar.n # remaining batches\n",
    "                num_batches_done = len(train_loader) - num_batches_done if num_batches_done <= len(train_loader) else len(train_loader)\n",
    "\n",
    "                last_loss = total_loss / num_batches_done if num_batches_done > 0 else float('inf')\n",
    "                self.save_checkpoint(current_training_epoch + 1, optimizer, last_loss, f'unsupervised_interrupted_checkpoint.pth')\n",
    "                break # Exit the epoch loop\n",
    "            \n",
    "            if training_interrupted:\n",
    "                break # Exit the outer loop if interrupted\n",
    "\n",
    "        if not training_interrupted and self.epochs: # Ensure training ran for at least one epoch\n",
    "            final_epoch_num = self.epochs[-1] \n",
    "            final_loss = self.train_losses[-1]\n",
    "            self.save_checkpoint(final_epoch_num, optimizer, final_loss, f'unsupervised_final_model.pth')\n",
    "            print(f\"Training complete! Final model saved after epoch {final_epoch_num}.\")\n",
    "        elif not self.epochs:\n",
    "             print(\"Training did not run for any epochs (e.g., num_epochs was 0 or loaded checkpoint was at target). No final model saved from this run.\")\n",
    "        else: # training_interrupted\n",
    "            print(\"Training was interrupted. Final model not saved as 'unsupervised_final_model.pth'. Interrupted checkpoint saved.\")\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def set_threshold(self, validation_loader, percentile=95):\n",
    "        \"\"\"\n",
    "        Set threshold for anomaly detection based on validation data\n",
    "        \n",
    "        Args:\n",
    "            validation_loader: Validation data loader (normal samples)\n",
    "            percentile: Percentile to set as threshold\n",
    "        \"\"\"\n",
    "        self.autoencoder.eval()\n",
    "        errors = []\n",
    "        \n",
    "        if len(validation_loader) == 0:\n",
    "            print(\"Warning: Validation loader is empty. Cannot set threshold. Using a default or previously set threshold.\")\n",
    "            if self.threshold is None: # if no threshold was ever set (e.g. from checkpoint)\n",
    "                self.threshold = 1.0 # Arbitrary default, should be tuned or error out\n",
    "                print(\"Setting threshold to arbitrary 1.0 due to empty validation set.\")\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in tqdm(validation_loader, desc=\"Computing threshold\"):\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                reconstruction_errors = self.autoencoder.get_reconstruction_error(X_batch)\n",
    "                errors.extend(reconstruction_errors.cpu().numpy())\n",
    "        \n",
    "        if not errors:\n",
    "            print(\"Warning: No reconstruction errors computed from validation set. Threshold not set. Using previous or default.\")\n",
    "            if self.threshold is None:\n",
    "                self.threshold = 1.0\n",
    "                print(\"Setting threshold to arbitrary 1.0.\")\n",
    "            return\n",
    "\n",
    "        self.threshold = np.percentile(errors, percentile)\n",
    "        print(f\"Threshold set to {self.threshold:.4f} based on {percentile}th percentile of validation errors.\")\n",
    "        \n",
    "    def predict(self, X, batch_size=4):\n",
    "        \"\"\"\n",
    "        Make predictions using the reconstruction error and apply Kalman filtering\n",
    "        with memory-efficient batch processing\n",
    "        \n",
    "        Args:\n",
    "            X: Input data tensor or numpy array\n",
    "            batch_size: Batch size for prediction to avoid OOM errors\n",
    "            \n",
    "        Returns:\n",
    "            anomalies: Binary anomaly predictions\n",
    "            filtered_scores: Continuous anomaly scores after Kalman filtering\n",
    "            raw_scores: Raw reconstruction errors\n",
    "        \"\"\"\n",
    "        # Adhering to the original script's structure for this part:\n",
    "        if self.threshold is None:\n",
    "            # The original script had an indented self.autoencoder.eval() here.\n",
    "            # This implies it's part of the if block.\n",
    "            self.autoencoder.eval()\n",
    "            # It's critical to note that predicting with a None threshold will cause issues later.\n",
    "            # A proper check should raise an error.\n",
    "            print(\"Warning: UnsupervisedAnomalyDetector.predict called with self.threshold = None. This will likely lead to errors or incorrect behavior.\")\n",
    "        \n",
    "        self.autoencoder.eval() # This was the unconditional eval call in the original script.\n",
    "\n",
    "        # A more robust check, though not in original, would be:\n",
    "        if self.threshold is None:\n",
    "             raise ValueError(\"Threshold is not set. Call set_threshold or load a checkpoint with a threshold before predicting.\")\n",
    "\n",
    "\n",
    "        if isinstance(X, np.ndarray):\n",
    "            total_samples = X.shape[0]\n",
    "        elif isinstance(X, torch.Tensor):\n",
    "            total_samples = X.size(0)\n",
    "        else:\n",
    "            raise TypeError(\"Input X must be a NumPy array or PyTorch Tensor.\")\n",
    "\n",
    "        if total_samples == 0:\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        all_errors = np.zeros(total_samples)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, total_samples, batch_size), desc=\"Predicting\", leave=False):\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    end_idx = min(i + batch_size, total_samples)\n",
    "                    if isinstance(X, np.ndarray):\n",
    "                        X_batch_np = X[i:end_idx]\n",
    "                        X_tensor = torch.tensor(X_batch_np, dtype=torch.float32).to(self.device)\n",
    "                    else: # X is already a Tensor\n",
    "                        X_tensor = X[i:end_idx].to(self.device)\n",
    "                    \n",
    "                    reconstruction_errors = self.autoencoder.get_reconstruction_error(X_tensor)\n",
    "                    all_errors[i:end_idx] = reconstruction_errors.cpu().numpy()\n",
    "                    \n",
    "                    del X_tensor, reconstruction_errors\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            filtered_scores = self.kalman_filter.filter(all_errors)\n",
    "            anomalies = filtered_scores > self.threshold # This will error if self.threshold is None\n",
    "            \n",
    "            return anomalies, filtered_scores, all_errors\n",
    "            \n",
    "        except Exception as e:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            raise e\n",
    "\n",
    "def train_unsupervised(df_train_faultfree, lookback=15, num_epochs=30, batch_size=16):\n",
    "    \"\"\"\n",
    "    Train unsupervised model and save the model and scaler\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training unsupervised anomaly detector\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        X_train_raw = df_train_faultfree.iloc[:, 3:].values\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "        \n",
    "        train_val_split_idx = int(0.9 * len(X_train_scaled))\n",
    "        X_train_for_model = X_train_scaled[:train_val_split_idx]\n",
    "        X_val_for_model = X_train_scaled[train_val_split_idx:]\n",
    "        \n",
    "        X_train_windows = create_time_series_windows(X_train_for_model, lookback, start_idx=0)\n",
    "        X_val_windows = create_time_series_windows(X_val_for_model, lookback, start_idx=0)\n",
    "        \n",
    "        # Handle case where window creation might yield empty tensors (e.g., if data is too short)\n",
    "        if X_train_windows.nelement() == 0 and X_val_windows.nelement() == 0:\n",
    "            print(\"Error: Not enough training/validation data to create any windows. Aborting training.\")\n",
    "            return None, None\n",
    "        if X_train_windows.nelement() == 0:\n",
    "            print(\"Warning: No training windows created. Check training data length and lookback.\")\n",
    "            # Potentially use validation windows for feature_dim if training is empty, though this is unusual.\n",
    "            if X_val_windows.nelement() == 0:\n",
    "                 print(\"Error: No validation windows either. Cannot determine feature dimension.\")\n",
    "                 return None, None\n",
    "\n",
    "        train_dataset = UnsupervisedDataset(X_train_windows)\n",
    "        val_dataset = UnsupervisedDataset(X_val_windows)\n",
    "        \n",
    "        # Use larger batch_size for validation loader if possible, but original used training batch_size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True if len(train_dataset) > batch_size else False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False) \n",
    "        \n",
    "        # Determine feature_dim robustly\n",
    "        if X_train_windows.nelement() > 0:\n",
    "            feature_dim = X_train_windows.shape[2]\n",
    "        elif X_val_windows.nelement() > 0: # Fallback to val windows if train is empty\n",
    "            feature_dim = X_val_windows.shape[2]\n",
    "            print(\"Warning: Using validation window features to determine model input dimension as training windows are empty.\")\n",
    "        else: # Fallback to raw data shape if no windows at all (should have been caught earlier)\n",
    "            feature_dim = df_train_faultfree.shape[1] - 3 \n",
    "            print(\"Critical Warning: No windows created. Using raw feature count for model input dimension. This may be incorrect.\")\n",
    "\n",
    "        detector = UnsupervisedAnomalyDetector(\n",
    "            input_dim=feature_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1, Q=1e-5, R=0.1\n",
    "        )\n",
    "        \n",
    "        del X_train_raw, X_train_scaled, X_train_for_model, X_val_for_model, X_train_windows, X_val_windows, train_dataset, val_dataset\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        optimizer = detector.train_model( \n",
    "            train_loader=train_loader, val_loader=val_loader if len(val_loader)>0 else None, num_epochs=num_epochs,\n",
    "            lr=1e-3, resume=True, save_interval=5\n",
    "        )\n",
    "        \n",
    "        if len(val_loader)>0:\n",
    "            detector.set_threshold(val_loader, percentile=95) \n",
    "        else: # No validation data to set threshold\n",
    "            print(\"Warning: Validation loader is empty. Cannot set threshold automatically.\")\n",
    "            # Attempt to load threshold from 'best_model' or 'final_model' if they exist and have one\n",
    "            # Or use a pre-defined default, or raise an error.\n",
    "            # For now, if it's None, predict() will raise error.\n",
    "            loaded_epoch, loaded_loss = detector.load_checkpoint(optimizer, filename='unsupervised_best_model.pth') # try loading best\n",
    "            if detector.threshold is None:\n",
    "                loaded_epoch, loaded_loss = detector.load_checkpoint(optimizer, filename='unsupervised_final_model.pth') # try final\n",
    "            if detector.threshold is None:\n",
    "                print(\"Could not set threshold from validation data or load from checkpoints. Manual setting or default needed.\")\n",
    "                # detector.threshold = some_default_value # e.g. np.percentile of reconstruction errors on a small normal test subset\n",
    "                                                       # Or raise error before evaluation loop.\n",
    "                # For now, allow it to be None and let predict() handle it.\n",
    "\n",
    "        try:\n",
    "            epoch_for_save = detector.epochs[-1] if detector.epochs else (detector.current_epoch + 1 if detector.current_epoch is not None and detector.current_epoch !=-1 else 0)\n",
    "            loss_for_save = (detector.val_losses[-1] if detector.val_losses else \n",
    "                            (detector.train_losses[-1] if detector.train_losses else 0.0))\n",
    "            detector.save_checkpoint(epoch_for_save, optimizer, loss_for_save, \n",
    "                                    f'unsupervised_final_model_with_threshold.pth')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save final model with threshold: {str(e)}\")\n",
    "        \n",
    "        # Save the scaler for later use\n",
    "        with open('scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "            print(\"StandardScaler saved to scaler.pkl\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Unsupervised model training run complete.\")\n",
    "        print(\"Model has been saved in the checkpoints directory.\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "        return detector, scaler\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in training function: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Main training function\n",
    "def main_training():\n",
    "    \"\"\"\n",
    "    Main function to train unsupervised model\n",
    "    \"\"\"\n",
    "    lookback = 15\n",
    "    num_epochs = 30  # Can be adjusted\n",
    "    batch_size = 16  # For training\n",
    "\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        detector, scaler = train_unsupervised(\n",
    "            df_train_faultfree,\n",
    "            lookback=lookback,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        return detector, scaler\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in main training function: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Only execute main function if script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    detector, scaler = main_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73c919-58fa-4ce8-aa89-45fe78ef0f38",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae7805d-57ac-41e4-b045-5cf7ff2ca5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Available fault types: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Loaded StandardScaler from scaler.pkl\n",
      "Loaded model from checkpoints\\unsupervised_final_model_with_threshold.pth\n",
      "Model threshold: 4.132619324082043e-05\n",
      "Threshold not set or invalid. Setting threshold from normal data.\n",
      "Computing threshold based on normal operation data (percentile=95)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing threshold: 100%|███████████████████████████████████████████████████████| 2493/2493 [00:07<00:00, 317.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.0000 based on 95th percentile of normal operation data\n",
      "Using threshold: 3.307684892206453e-05\n",
      "Will evaluate for fault types: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 0\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only one class present in y_true for fault type 0. ROC AUC and PR AUC are not defined.\n",
      "\n",
      "Results for Fault Type 0:\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1 Score:  0.0000\n",
      "  Accuracy:  0.9752\n",
      "  ROC-AUC:   nan\n",
      "  PR-AUC:    nan\n",
      "\n",
      "Confusion Matrix Details for Fault Type 0:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 0 (0.0%)\n",
      "  False Positives (FP): 11906 (2.5%)\n",
      "  True Negatives (TN): 468080 (97.5%)\n",
      "  False Negatives (FN): 0 (0.0%)\n",
      "  TN Rate (Specificity): 0.9752\n",
      "  FP Rate: 0.0248\n",
      "Visualizations for Fault Type 0 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 1:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 1:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479830 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 10 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 1 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 2:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 2:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479820 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 20 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 2 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 4:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9725\n",
      "  F1 Score:  0.9861\n",
      "  Accuracy:  0.9725\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 4:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 466644 (97.2%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 13196 (2.7%)\n",
      "  TP Rate (Recall): 0.9725\n",
      "  FN Rate: 0.0275\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 4 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 5\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 5:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 5:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479832 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 8 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 5 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 6\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 6:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 6:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479840 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 0 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 6 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 7\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 7:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 7:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479839 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 1 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 7 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 8\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 8:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9999\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  0.9999\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 8:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479816 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 24 (0.0%)\n",
      "  TP Rate (Recall): 0.9999\n",
      "  FN Rate: 0.0001\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 8 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 10:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9999\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  0.9999\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 10:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479815 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 25 (0.0%)\n",
      "  TP Rate (Recall): 0.9999\n",
      "  FN Rate: 0.0001\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 10 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 11\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 11:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9999\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  0.9999\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 11:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479814 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 26 (0.0%)\n",
      "  TP Rate (Recall): 0.9999\n",
      "  FN Rate: 0.0001\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 11 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 12\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 12:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 12:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479830 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 10 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 12 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 13\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 13:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9998\n",
      "  F1 Score:  0.9999\n",
      "  Accuracy:  0.9998\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 13:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479765 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 75 (0.0%)\n",
      "  TP Rate (Recall): 0.9998\n",
      "  FN Rate: 0.0002\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 13 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 14\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 14:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 14:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479838 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 2 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 14 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 16\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 16:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 16:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479825 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 15 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 16 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 17\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 17:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9998\n",
      "  F1 Score:  0.9999\n",
      "  Accuracy:  0.9998\n",
      "  ROC-AUC:   0.9999\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 17:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479757 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 83 (0.0%)\n",
      "  TP Rate (Recall): 0.9998\n",
      "  FN Rate: 0.0002\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 17 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 18\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 18:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9999\n",
      "  F1 Score:  0.9999\n",
      "  Accuracy:  0.9999\n",
      "  ROC-AUC:   0.9999\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 18:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479769 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 71 (0.0%)\n",
      "  TP Rate (Recall): 0.9999\n",
      "  FN Rate: 0.0001\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 18 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 19\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 19:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 19:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479821 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 19 (0.0%)\n",
      "  TP Rate (Recall): 1.0000\n",
      "  FN Rate: 0.0000\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 19 saved to fault_evaluation_results/\n",
      "\n",
      "================================================================================\n",
      "Evaluating detector on Fault Type 20\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fault Type 20:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9999\n",
      "  F1 Score:  0.9999\n",
      "  Accuracy:  0.9999\n",
      "  ROC-AUC:   1.0000\n",
      "  PR-AUC:    1.0000\n",
      "\n",
      "Confusion Matrix Details for Fault Type 20:\n",
      "  Total samples: 479986\n",
      "  True Positives (TP): 479786 (100.0%)\n",
      "  False Positives (FP): 0 (0.0%)\n",
      "  True Negatives (TN): 146 (0.0%)\n",
      "  False Negatives (FN): 54 (0.0%)\n",
      "  TP Rate (Recall): 0.9999\n",
      "  FN Rate: 0.0001\n",
      "  TN Rate (Specificity): 1.0000\n",
      "  FP Rate: 0.0000\n",
      "Visualizations for Fault Type 20 saved to fault_evaluation_results/\n",
      "\n",
      "Summary of results for all evaluated fault types:\n",
      "    Fault Type  Precision    Recall  F1 Score  Accuracy   ROC-AUC  PR-AUC  \\\n",
      "0            0        0.0  0.000000  0.000000  0.975195       NaN     NaN   \n",
      "1            1        1.0  0.999979  0.999990  0.999979  0.999995     1.0   \n",
      "2            2        1.0  0.999958  0.999979  0.999958  0.999990     1.0   \n",
      "3            4        1.0  0.972499  0.986058  0.972508  0.999967     1.0   \n",
      "4            5        1.0  0.999983  0.999992  0.999983  0.999999     1.0   \n",
      "5            6        1.0  1.000000  1.000000  1.000000  1.000000     1.0   \n",
      "6            7        1.0  0.999998  0.999999  0.999998  1.000000     1.0   \n",
      "7            8        1.0  0.999950  0.999975  0.999950  0.999983     1.0   \n",
      "8           10        1.0  0.999948  0.999974  0.999948  0.999981     1.0   \n",
      "9           11        1.0  0.999946  0.999973  0.999946  0.999989     1.0   \n",
      "10          12        1.0  0.999979  0.999990  0.999979  0.999995     1.0   \n",
      "11          13        1.0  0.999844  0.999922  0.999844  0.999953     1.0   \n",
      "12          14        1.0  0.999996  0.999998  0.999996  0.999999     1.0   \n",
      "13          16        1.0  0.999969  0.999984  0.999969  0.999990     1.0   \n",
      "14          17        1.0  0.999827  0.999914  0.999827  0.999926     1.0   \n",
      "15          18        1.0  0.999852  0.999926  0.999852  0.999939     1.0   \n",
      "16          19        1.0  0.999960  0.999980  0.999960  0.999997     1.0   \n",
      "17          20        1.0  0.999887  0.999944  0.999887  0.999957     1.0   \n",
      "\n",
      "        TP     FP      TN     FN  \n",
      "0        0  11906  468080      0  \n",
      "1   479830      0     146     10  \n",
      "2   479820      0     146     20  \n",
      "3   466644      0     146  13196  \n",
      "4   479832      0     146      8  \n",
      "5   479840      0     146      0  \n",
      "6   479839      0     146      1  \n",
      "7   479816      0     146     24  \n",
      "8   479815      0     146     25  \n",
      "9   479814      0     146     26  \n",
      "10  479830      0     146     10  \n",
      "11  479765      0     146     75  \n",
      "12  479838      0     146      2  \n",
      "13  479825      0     146     15  \n",
      "14  479757      0     146     83  \n",
      "15  479769      0     146     71  \n",
      "16  479821      0     146     19  \n",
      "17  479786      0     146     54  \n",
      "\n",
      "All summary visualizations saved to fault_evaluation_results/\n",
      "\n",
      "Testing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import torch\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Set device for PyTorch (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load datasets\n",
    "df_test_faultfree = pyreadr.read_r(\"TEP_FaultFree_Testing.RData\")['fault_free_testing']\n",
    "df_test_faulty = pyreadr.read_r(\"TEP_Faulty_Testing.RData\")['faulty_testing']\n",
    "\n",
    "# Get all fault types\n",
    "fault_types = sorted(df_test_faulty['faultNumber'].unique())\n",
    "print(f\"Available fault types: {fault_types}\")\n",
    "\n",
    "# Kalman Filter implementation for smoothing predictions (copied from original)\n",
    "class KalmanFilter:\n",
    "    \"\"\"\n",
    "    Simple implementation of Kalman filter for 1D signal smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_x, Q=1e-4, R=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Kalman filter\n",
    "\n",
    "        Args:\n",
    "            dim_x: State dimension\n",
    "            Q: Process noise covariance\n",
    "            R: Measurement noise covariance\n",
    "        \"\"\"\n",
    "        self.dim_x = dim_x\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "\n",
    "    def filter(self, y_noisy):\n",
    "        \"\"\"\n",
    "        Apply Kalman filtering to noisy observations\n",
    "\n",
    "        Args:\n",
    "            y_noisy: Noisy observations\n",
    "\n",
    "        Returns:\n",
    "            x_est: Filtered (smoothed) state estimates\n",
    "        \"\"\"\n",
    "        n = len(y_noisy)\n",
    "        x_est = np.zeros(n)  # State estimates\n",
    "        P = np.zeros(n)      # Error covariance\n",
    "\n",
    "        # Initial state and covariance\n",
    "        if n > 0:\n",
    "            x_est[0] = y_noisy[0]\n",
    "            P[0] = 1.0\n",
    "        else:\n",
    "            return x_est # Return empty if no data\n",
    "\n",
    "        for k in range(1, n):\n",
    "            # Prediction step\n",
    "            x_pred = x_est[k-1]      # State prediction\n",
    "            P_pred = P[k-1] + self.Q  # Covariance prediction\n",
    "\n",
    "            # Update step with measurement\n",
    "            K = P_pred / (P_pred + self.R)  # Kalman gain\n",
    "            x_est[k] = x_pred + K * (y_noisy[k] - x_pred)  # State update\n",
    "            P[k] = (1 - K) * P_pred    # Covariance update\n",
    "\n",
    "        return x_est\n",
    "\n",
    "# Create PyTorch dataset - exactly as in original code\n",
    "class UnsupervisedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for unsupervised learning (no labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# Create sliding window time series input - using original implementation\n",
    "def create_time_series_windows(X, lookback=15, max_samples=None, start_idx=0):\n",
    "    \"\"\"\n",
    "    Create sliding window dataset for unsupervised learning\n",
    "    with memory efficiency optimizations and proper handling of time order\n",
    "    \"\"\"\n",
    "    # Calculate valid range for window creation (respecting time boundaries)\n",
    "    n_potential_samples = len(X) - lookback + 1 - start_idx\n",
    "    # Ensure n_potential_samples is not negative\n",
    "    n_samples = max(0, n_potential_samples)\n",
    "\n",
    "    # Handle max_samples limit if specified\n",
    "    if max_samples is not None and n_samples > max_samples:\n",
    "        n_samples = max_samples\n",
    "    \n",
    "    # Pre-allocate memory for efficiency\n",
    "    n_features = X.shape[1]\n",
    "    X_windows = np.zeros((n_samples, lookback, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Process in batches to save memory\n",
    "    for i in tqdm(range(n_samples), desc=\"Creating windows\", miniters=int(n_samples/100) if n_samples > 100 else 1, leave=False):\n",
    "        actual_idx = start_idx + i\n",
    "        X_windows[i] = X[actual_idx : actual_idx + lookback]\n",
    "    \n",
    "    return torch.tensor(X_windows, dtype=torch.float32)\n",
    "\n",
    "# Positional Encoding for Transformer model - exactly as in original code\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for Transformer model\n",
    "    Adds information about position of tokens in the sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer-based autoencoder for unsupervised anomaly detection - exactly as in original code\n",
    "class TransformerAutoencoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based autoencoder for unsupervised anomaly detection\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Transformer autoencoder\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            d_model: Hidden dimension for the model\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input embedding\n",
    "        self.embedding = torch.nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=256, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Decoder to reconstruct the input\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            x_reconstructed: Reconstructed input\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Embedding and scale\n",
    "        x_embedded = self.embedding(x) * np.sqrt(self.embedding.out_features)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x_embedded = self.pos_encoder(x_embedded)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x_encoded = self.transformer_encoder(x_embedded)\n",
    "        \n",
    "        # Decode each time step\n",
    "        x_reconstructed = torch.zeros_like(x)\n",
    "        for i in range(seq_len):\n",
    "            x_reconstructed[:, i] = self.decoder(x_encoded[:, i])\n",
    "            \n",
    "        return x_reconstructed\n",
    "    \n",
    "    def get_reconstruction_error(self, x):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error (MSE) for anomaly detection\n",
    "        with memory-efficient implementation\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            error: Reconstruction error\n",
    "        \"\"\"\n",
    "        # Process one sample at a time if batch size is 1\n",
    "        if x.size(0) == 1:\n",
    "            x_reconstructed = self(x)\n",
    "            error = torch.mean(torch.pow(x - x_reconstructed, 2), dim=(1, 2))\n",
    "            return error\n",
    "        \n",
    "        # For larger batches, split into smaller sub-batches if needed\n",
    "        batch_size = x.size(0)\n",
    "        if batch_size > 4:  # If batch is large\n",
    "            errors = []\n",
    "            sub_batch_size = 4  # Process 4 samples at a time\n",
    "            \n",
    "            for i in range(0, batch_size, sub_batch_size):\n",
    "                end_idx = min(i + sub_batch_size, batch_size)\n",
    "                sub_batch = x[i:end_idx]\n",
    "                sub_reconstructed = self(sub_batch)\n",
    "                sub_error = torch.mean(torch.pow(sub_batch - sub_reconstructed, 2), dim=(1, 2))\n",
    "                errors.append(sub_error)\n",
    "                \n",
    "                # Free memory\n",
    "                del sub_batch, sub_reconstructed\n",
    "                \n",
    "            return torch.cat(errors)\n",
    "        else:\n",
    "            # Standard processing for reasonable batch sizes\n",
    "            x_reconstructed = self(x)\n",
    "            error = torch.mean(torch.pow(x - x_reconstructed, 2), dim=(1, 2))\n",
    "            return error\n",
    "\n",
    "# Neural network anomaly detector with reconstruction-based approach - exactly as in original code\n",
    "class UnsupervisedAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Unsupervised anomaly detector based on reconstruction error\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1, Q=1e-4, R=0.1):\n",
    "        \"\"\"\n",
    "        Initialize detector\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            d_model: Hidden dimension for transformer\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of transformer layers\n",
    "            dropout: Dropout probability\n",
    "            Q: Process noise for Kalman filter\n",
    "            R: Measurement noise for Kalman filter\n",
    "        \"\"\"\n",
    "        self.autoencoder = TransformerAutoencoder(input_dim, d_model, nhead, num_layers, dropout)\n",
    "        self.kalman_filter = KalmanFilter(1, Q, R)\n",
    "        self.device = device\n",
    "        self.autoencoder.to(self.device)\n",
    "        \n",
    "        # Training related variables\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = []\n",
    "        self.current_epoch = 0 # Stores the last completed epoch (0-indexed)\n",
    "        \n",
    "        # Threshold for anomaly detection\n",
    "        self.threshold = None\n",
    "        \n",
    "        # Checkpoint directory\n",
    "        self.checkpoint_dir = 'checkpoints'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, epoch, optimizer, loss, filename=None):\n",
    "        \"\"\"\n",
    "        Save model checkpoint - safely handles None optimizer\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch (1-based for saving convention)\n",
    "            optimizer: Optimizer state (can be None for inference-only checkpoints)\n",
    "            loss: Current loss value\n",
    "            filename: Name of the checkpoint file\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            filename = f'unsupervised_checkpoint.pth'\n",
    "        path = os.path.join(self.checkpoint_dir, filename)\n",
    "    \n",
    "        # Create checkpoint dictionary\n",
    "        checkpoint_dict = {\n",
    "            'epoch': epoch, # This is the 1-based epoch number\n",
    "            'model_state_dict': self.autoencoder.state_dict(),\n",
    "            'loss': loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'epochs': self.epochs, # List of 1-based epoch numbers trained\n",
    "            'current_epoch': self.current_epoch, # Last completed 0-indexed epoch\n",
    "            'threshold': self.threshold\n",
    "        }\n",
    "    \n",
    "        # Only add optimizer state if it's not None\n",
    "        if optimizer is not None:\n",
    "            try:\n",
    "                checkpoint_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "            except AttributeError:\n",
    "                print(\"Warning: optimizer does not have state_dict method, not saving optimizer state\")\n",
    "    \n",
    "        torch.save(checkpoint_dict, path)\n",
    "        print(f\"Checkpoint saved at {path}\")\n",
    "        \n",
    "    def load_checkpoint(self, optimizer, filename=None):\n",
    "        \"\"\"\n",
    "        Load model checkpoint\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer to load state into (can be None)\n",
    "            filename: Name of the checkpoint file\n",
    "            \n",
    "        Returns:\n",
    "            start_epoch: Epoch to start training from (0-indexed for loop)\n",
    "            loss: Loss value of the checkpoint\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            filename = f'unsupervised_checkpoint.pth'\n",
    "        path = os.path.join(self.checkpoint_dir, filename)\n",
    "        if os.path.exists(path):\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                \n",
    "            self.train_losses = checkpoint.get('train_losses', [])\n",
    "            self.val_losses = checkpoint.get('val_losses', [])\n",
    "            self.epochs = checkpoint.get('epochs', [])\n",
    "            self.current_epoch = checkpoint.get('current_epoch', 0) # Last completed 0-indexed epoch\n",
    "            self.threshold = checkpoint.get('threshold', None)\n",
    "            \n",
    "            # 'epoch' in checkpoint is 1-based, so start_epoch for training loop is this value\n",
    "            # current_epoch (0-indexed) is more reliable for determining next epoch.\n",
    "            start_epoch_for_loop = self.current_epoch + 1 if self.epochs else 0\n",
    "\n",
    "            print(f\"Checkpoint loaded from {path}, resuming from epoch {start_epoch_for_loop}\")\n",
    "            return start_epoch_for_loop, checkpoint.get('loss', float('inf'))\n",
    "        else:\n",
    "            print(f\"No checkpoint found at {path}, starting from scratch\")\n",
    "            return 0, float('inf') # Start from epoch 0 (0-indexed)\n",
    "        \n",
    "    def predict(self, X, batch_size=4):\n",
    "        \"\"\"\n",
    "        Make predictions using the reconstruction error and apply Kalman filtering\n",
    "        with memory-efficient batch processing\n",
    "        \n",
    "        Args:\n",
    "            X: Input data tensor or numpy array\n",
    "            batch_size: Batch size for prediction to avoid OOM errors\n",
    "            \n",
    "        Returns:\n",
    "            anomalies: Binary anomaly predictions\n",
    "            filtered_scores: Continuous anomaly scores after Kalman filtering\n",
    "            raw_scores: Raw reconstruction errors\n",
    "        \"\"\"\n",
    "        # Using same logic as original code\n",
    "        if self.threshold is None:\n",
    "            self.autoencoder.eval()\n",
    "            print(\"Warning: UnsupervisedAnomalyDetector.predict called with self.threshold = None. Setting default threshold of 0.1\")\n",
    "            self.threshold = 0.1  # Set a default threshold instead of raising an error\n",
    "        \n",
    "        self.autoencoder.eval()\n",
    "\n",
    "        if isinstance(X, np.ndarray):\n",
    "            total_samples = X.shape[0]\n",
    "        elif isinstance(X, torch.Tensor):\n",
    "            total_samples = X.size(0)\n",
    "        else:\n",
    "            raise TypeError(\"Input X must be a NumPy array or PyTorch Tensor.\")\n",
    "\n",
    "        if total_samples == 0:\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        all_errors = np.zeros(total_samples)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, total_samples, batch_size), desc=\"Predicting\", leave=False):\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    end_idx = min(i + batch_size, total_samples)\n",
    "                    if isinstance(X, np.ndarray):\n",
    "                        X_batch_np = X[i:end_idx]\n",
    "                        X_tensor = torch.tensor(X_batch_np, dtype=torch.float32).to(self.device)\n",
    "                    else: # X is already a Tensor\n",
    "                        X_tensor = X[i:end_idx].to(self.device)\n",
    "                    \n",
    "                    reconstruction_errors = self.autoencoder.get_reconstruction_error(X_tensor)\n",
    "                    all_errors[i:end_idx] = reconstruction_errors.cpu().numpy()\n",
    "                    \n",
    "                    del X_tensor, reconstruction_errors\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            filtered_scores = self.kalman_filter.filter(all_errors)\n",
    "            anomalies = filtered_scores > self.threshold\n",
    "            \n",
    "            return anomalies, filtered_scores, all_errors\n",
    "            \n",
    "        except Exception as e:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            raise e\n",
    "\n",
    "# Enhanced evaluate_fault_detector function - modified to match original implementation\n",
    "def evaluate_fault_detector(detector, fault_type, df_test_faultfree, df_test_faulty, scaler, lookback=15, batch_size=4, \n",
    "                           save_plots=True, output_dir='results'):\n",
    "    \"\"\"\n",
    "    Enhanced evaluate anomaly detector with detailed metrics and visualizations\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating detector on Fault Type {fault_type}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        X_test_data_raw = None\n",
    "        y_true_raw = None\n",
    "\n",
    "        if fault_type == 0:\n",
    "            # Use only fault-free test data\n",
    "            X_test_data_raw = df_test_faultfree.iloc[:, 3:].values\n",
    "            y_true_raw = np.zeros(len(X_test_data_raw))\n",
    "            if len(X_test_data_raw) == 0:\n",
    "                print(f\"No fault-free test data available. Skipping evaluation for fault type 0.\")\n",
    "                return {'fault_type': fault_type, 'error': 'No fault-free test data', \n",
    "                        'precision': 0, 'recall': 0, 'f1': 0, 'accuracy': 0, 'roc_auc': np.nan, 'pr_auc': np.nan}\n",
    "        else:\n",
    "            # Use specific fault data from faulty test set\n",
    "            test_fault_data_df = df_test_faulty[df_test_faulty['faultNumber'] == fault_type]\n",
    "            if len(test_fault_data_df) == 0:\n",
    "                print(f\"No test data found for fault type {fault_type}. Skipping evaluation...\")\n",
    "                return {'fault_type': fault_type, 'error': f'No test data for fault {fault_type}', \n",
    "                        'precision': 0, 'recall': 0, 'f1': 0, 'accuracy': 0, 'roc_auc': np.nan, 'pr_auc': np.nan}\n",
    "\n",
    "            X_test_data_raw = test_fault_data_df.iloc[:, 3:].values\n",
    "            y_true_raw = np.ones(len(X_test_data_raw))\n",
    "            # First 160 samples in TEP faulty datasets are normal before fault inception\n",
    "            if len(y_true_raw) >= 160: # Ensure there are enough samples\n",
    "                 y_true_raw[:160] = 0\n",
    "            else: # If dataset is shorter than 160, all are normal if fault hasn't started.\n",
    "                  pass\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test_data_raw)\n",
    "        # The number of windows will be len(X_test_scaled) - lookback + 1 for start_idx=0\n",
    "        X_test_windows = create_time_series_windows(X_test_scaled, lookback, start_idx=0)\n",
    "\n",
    "        if X_test_windows.nelement() == 0: # Check if tensor is empty\n",
    "            print(f\"Not enough data to create windows for fault type {fault_type} with lookback {lookback}. Skipping.\")\n",
    "            return {'fault_type': fault_type, 'error': 'Not enough data for windows', \n",
    "                    'precision': 0, 'recall': 0, 'f1': 0, 'accuracy': 0, 'roc_auc': np.nan, 'pr_auc': np.nan}\n",
    "\n",
    "        # Adjust labels: label for window X[i:i+lookback-1] corresponds to label y[i+lookback-1]\n",
    "        # This means we need labels starting from index (lookback-1) to match the windows\n",
    "        y_true_for_windows = y_true_raw[lookback-1:lookback-1+len(X_test_windows)]\n",
    "        \n",
    "        num_windows = len(X_test_windows)\n",
    "        \n",
    "        if len(y_true_for_windows) < num_windows:\n",
    "            print(f\"Warning: Original labels too short ({len(y_true_for_windows)}) for {num_windows} windows for fault {fault_type}. Trimming windows.\")\n",
    "            X_test_windows = X_test_windows[:len(y_true_for_windows)]\n",
    "            num_windows = len(X_test_windows)\n",
    "            if num_windows == 0:\n",
    "                 print(f\"No windows left after label alignment for fault {fault_type}.\")\n",
    "                 return {'fault_type': fault_type, 'error': 'No windows after label alignment', \n",
    "                        'precision': 0, 'recall': 0, 'f1': 0, 'accuracy': 0, 'roc_auc': np.nan, 'pr_auc': np.nan}\n",
    "\n",
    "        # Get predictions and scores\n",
    "        anomalies_pred, filtered_scores, raw_scores = detector.predict(\n",
    "            X_test_windows, \n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Calculate confusion matrix components\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_for_windows, anomalies_pred, labels=[0, 1]).ravel()\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true_for_windows, anomalies_pred, average='binary', zero_division=0)\n",
    "        accuracy = accuracy_score(y_true_for_windows, anomalies_pred)\n",
    "        \n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "\n",
    "        if len(np.unique(y_true_for_windows)) > 1: # Check if more than one class in true labels\n",
    "            if len(y_true_for_windows) == len(filtered_scores):\n",
    "                try:\n",
    "                    roc_auc = roc_auc_score(y_true_for_windows, filtered_scores)\n",
    "                    precision_curve_vals, recall_curve_vals, _ = precision_recall_curve(y_true_for_windows, filtered_scores)\n",
    "                    pr_auc = auc(recall_curve_vals, precision_curve_vals)\n",
    "                except ValueError as e_auc: # Handles cases like all scores being identical for one class\n",
    "                    print(f\"Could not calculate ROC/PR AUC for fault {fault_type}: {e_auc}\")\n",
    "            else:\n",
    "                print(f\"Warning: Length mismatch for ROC/PR AUC calculation for fault {fault_type}. True: {len(y_true_for_windows)}, Scores: {len(filtered_scores)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Only one class present in y_true for fault type {fault_type}. ROC AUC and PR AUC are not defined.\")\n",
    "\n",
    "        # Calculate detailed metrics\n",
    "        total_predictions = len(y_true_for_windows)\n",
    "        total_actual_positives = np.sum(y_true_for_windows)\n",
    "        total_actual_negatives = total_predictions - total_actual_positives\n",
    "        \n",
    "        results = {\n",
    "            'fault_type': fault_type,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'true_negatives': tn,\n",
    "            'false_negatives': fn,\n",
    "            'total_actual_positives': total_actual_positives,\n",
    "            'total_actual_negatives': total_actual_negatives,\n",
    "            'total_predictions': total_predictions\n",
    "        }\n",
    "\n",
    "        # Print detailed results including confusion matrix details\n",
    "        print(f\"\\nResults for Fault Type {fault_type}:\")\n",
    "        print(f\"  Precision: {results['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {results['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {results['f1']:.4f}\")\n",
    "        print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n",
    "        print(f\"  ROC-AUC:   {results['roc_auc']:.4f}\")\n",
    "        print(f\"  PR-AUC:    {results['pr_auc']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix Details for Fault Type {fault_type}:\")\n",
    "        print(f\"  Total samples: {total_predictions}\")\n",
    "        print(f\"  True Positives (TP): {tp} ({tp/total_predictions*100:.1f}%)\")\n",
    "        print(f\"  False Positives (FP): {fp} ({fp/total_predictions*100:.1f}%)\")\n",
    "        print(f\"  True Negatives (TN): {tn} ({tn/total_predictions*100:.1f}%)\")\n",
    "        print(f\"  False Negatives (FN): {fn} ({fn/total_predictions*100:.1f}%)\")\n",
    "        \n",
    "        if total_actual_positives > 0:\n",
    "            print(f\"  TP Rate (Recall): {tp/total_actual_positives:.4f}\")\n",
    "            print(f\"  FN Rate: {fn/total_actual_positives:.4f}\")\n",
    "        if total_actual_negatives > 0:\n",
    "            print(f\"  TN Rate (Specificity): {tn/total_actual_negatives:.4f}\")\n",
    "            print(f\"  FP Rate: {fp/total_actual_negatives:.4f}\")\n",
    "\n",
    "        if save_plots and len(filtered_scores) > 0:\n",
    "            # Save visualizations\n",
    "            \n",
    "            # Visualization of filtered scores and predictions\n",
    "            # Similar to original but with enhanced formatting\n",
    "            max_points = 1000\n",
    "            if len(filtered_scores) > max_points:\n",
    "                idx = np.linspace(0, len(filtered_scores)-1, max_points, dtype=int)\n",
    "                vis_filtered_scores = filtered_scores[idx]\n",
    "                vis_raw_scores = raw_scores[idx]\n",
    "                vis_anomalies = anomalies_pred[idx]\n",
    "                vis_y_test = y_true_for_windows[idx]\n",
    "            else:\n",
    "                vis_filtered_scores = filtered_scores\n",
    "                vis_raw_scores = raw_scores\n",
    "                vis_anomalies = anomalies_pred\n",
    "                vis_y_test = y_true_for_windows\n",
    "            \n",
    "            plt.figure(figsize=(16, 10))\n",
    "            \n",
    "            # Top subplot: Reconstruction errors and filtered scores\n",
    "            plt.subplot(3, 1, 1)\n",
    "            plt.plot(vis_raw_scores, 'y-', alpha=0.3, label='Raw Reconstruction Error')\n",
    "            plt.plot(vis_filtered_scores, 'b-', linewidth=1.5, label='Filtered Anomaly Score')\n",
    "            \n",
    "            # Mark detected anomalies on the filtered score plot\n",
    "            anomaly_indices = np.where(vis_anomalies)[0]\n",
    "            if len(anomaly_indices) > 0:\n",
    "                plt.scatter(anomaly_indices, vis_filtered_scores[anomaly_indices], \n",
    "                           color='r', s=20, marker='x', label='Detected Anomalies')\n",
    "            \n",
    "            # Add threshold line\n",
    "            if detector.threshold is not None:\n",
    "                plt.axhline(detector.threshold, color='k', linestyle='--', \n",
    "                           label=f'Threshold = {detector.threshold:.4f}')\n",
    "            \n",
    "            plt.title(f'Fault Type {fault_type} - Reconstruction Errors and Anomaly Scores', fontsize=14)\n",
    "            plt.ylabel('Error Magnitude')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(loc='upper right')\n",
    "            \n",
    "            # Middle subplot: True vs Predicted Labels\n",
    "            plt.subplot(3, 1, 2)\n",
    "            plt.plot(vis_y_test, 'g-', linewidth=2, label='True Labels')\n",
    "            plt.plot(vis_anomalies, 'r--', linewidth=1.5, label='Predicted Labels')\n",
    "            \n",
    "            # Highlight false positives (blue) and false negatives (orange)\n",
    "            fp_indices = np.where((vis_y_test == 0) & (vis_anomalies == 1))[0]\n",
    "            fn_indices = np.where((vis_y_test == 1) & (vis_anomalies == 0))[0]\n",
    "            \n",
    "            if len(fp_indices) > 0:\n",
    "                plt.scatter(fp_indices, vis_anomalies[fp_indices], color='blue', s=30, \n",
    "                           marker='o', label='False Positives', alpha=0.5)\n",
    "            \n",
    "            if len(fn_indices) > 0:\n",
    "                plt.scatter(fn_indices, vis_y_test[fn_indices], color='orange', s=30, \n",
    "                           marker='o', label='False Negatives', alpha=0.5)\n",
    "                \n",
    "            plt.title(f'True vs Predicted Labels', fontsize=14)\n",
    "            plt.ylabel('Label (0=Normal, 1=Fault)')\n",
    "            plt.ylim(-0.1, 1.1)\n",
    "            plt.yticks([0, 1])\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(loc='upper right')\n",
    "            \n",
    "            # Bottom subplot: Error distribution by label\n",
    "            plt.subplot(3, 1, 3)\n",
    "            \n",
    "            # Create bins for the histogram\n",
    "            bins = np.linspace(0, max(vis_filtered_scores) * 1.1, 50)\n",
    "            \n",
    "            # Separate scores by true label\n",
    "            normal_indices = np.where(vis_y_test == 0)[0]\n",
    "            fault_indices = np.where(vis_y_test == 1)[0]\n",
    "            \n",
    "            if len(normal_indices) > 0:\n",
    "                normal_scores = vis_filtered_scores[normal_indices]\n",
    "                plt.hist(normal_scores, bins=bins, alpha=0.6, color='green', \n",
    "                        label=f'Normal ({len(normal_indices)} samples)')\n",
    "            \n",
    "            if len(fault_indices) > 0:\n",
    "                fault_scores = vis_filtered_scores[fault_indices]\n",
    "                plt.hist(fault_scores, bins=bins, alpha=0.6, color='red', \n",
    "                        label=f'Fault ({len(fault_indices)} samples)')\n",
    "            \n",
    "            # Add threshold line\n",
    "            if detector.threshold is not None:\n",
    "                plt.axvline(detector.threshold, color='k', linestyle='--', \n",
    "                           label=f'Threshold = {detector.threshold:.4f}')\n",
    "            \n",
    "            plt.title('Distribution of Filtered Scores by True Label', fontsize=14)\n",
    "            plt.xlabel('Filtered Anomaly Score')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(loc='upper right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/fault_{fault_type}_detailed_eval.png', dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Visualizations for Fault Type {fault_type} saved to {output_dir}/\")\n",
    "        \n",
    "        # Clean up memory\n",
    "        del X_test_data_raw, X_test_scaled, X_test_windows\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating fault type {fault_type}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return {'fault_type': fault_type, 'error': str(e), \n",
    "                'precision': 0, 'recall': 0, 'f1': 0, 'accuracy': 0, 'roc_auc': np.nan, 'pr_auc': np.nan}\n",
    "\n",
    "def load_model_and_scaler():\n",
    "    \"\"\"\n",
    "    Load the trained model and scaler from files\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load scaler\n",
    "    try:\n",
    "        with open('scaler.pkl', 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"Loaded StandardScaler from scaler.pkl\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: scaler.pkl file not found. Please run training first.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Determine feature dimension from test data\n",
    "    feature_dim = df_test_faultfree.iloc[:, 3:].shape[1]\n",
    "    \n",
    "    # Create detector instance with same parameters as original\n",
    "    detector = UnsupervisedAnomalyDetector(\n",
    "        input_dim=feature_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1, Q=1e-5, R=0.1\n",
    "    )\n",
    "    \n",
    "    # Load model checkpoints\n",
    "    try:\n",
    "        # First try to load the checkpoint with threshold\n",
    "        checkpoint_path = os.path.join('checkpoints', 'unsupervised_final_model_with_threshold.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            detector.autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "            detector.threshold = checkpoint.get('threshold', None)\n",
    "            print(f\"Loaded model from {checkpoint_path}\")\n",
    "            print(f\"Model threshold: {detector.threshold}\")\n",
    "            return detector, scaler\n",
    "            \n",
    "        # Next try best model\n",
    "        checkpoint_path = os.path.join('checkpoints', 'unsupervised_best_model.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            detector.autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "            detector.threshold = checkpoint.get('threshold', None)\n",
    "            print(f\"Loaded model from {checkpoint_path}\")\n",
    "            print(f\"Model threshold: {detector.threshold}\")\n",
    "            return detector, scaler\n",
    "        \n",
    "        # Then try final model\n",
    "        checkpoint_path = os.path.join('checkpoints', 'unsupervised_final_model.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            detector.autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "            detector.threshold = checkpoint.get('threshold', None)\n",
    "            print(f\"Loaded model from {checkpoint_path}\")\n",
    "            print(f\"Model threshold: {detector.threshold}\")\n",
    "            return detector, scaler\n",
    "        \n",
    "        # If none of the above found, try any checkpoint\n",
    "        for file in os.listdir('checkpoints'):\n",
    "            if file.endswith('.pth'):\n",
    "                checkpoint_path = os.path.join('checkpoints', file)\n",
    "                checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "                detector.autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "                detector.threshold = checkpoint.get('threshold', None)\n",
    "                print(f\"Loaded model from {checkpoint_path}\")\n",
    "                print(f\"Model threshold: {detector.threshold}\")\n",
    "                return detector, scaler\n",
    "        \n",
    "        print(\"Error: No model checkpoint found in the checkpoints directory.\")\n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def set_threshold_from_normal_data(detector, scaler, lookback=15, percentile=95, batch_size=2):\n",
    "    \"\"\"\n",
    "    Set threshold from normal operation data - implements same logic as in the original code\n",
    "    \"\"\"\n",
    "    print(f\"Computing threshold based on normal operation data (percentile={percentile})...\")\n",
    "    \n",
    "    # Use a portion of fault-free test data to calculate threshold\n",
    "    max_samples = 5000  # Limit samples to avoid memory issues\n",
    "    X_test_normal = df_test_faultfree.iloc[:, 3:].values\n",
    "    if len(X_test_normal) > max_samples:\n",
    "        X_test_normal = X_test_normal[:max_samples]\n",
    "    \n",
    "    X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "    X_test_normal_windows = create_time_series_windows(X_test_normal_scaled, lookback)\n",
    "    \n",
    "    errors = []\n",
    "    detector.autoencoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(X_test_normal_windows), batch_size), desc=\"Computing threshold\"):\n",
    "            end_idx = min(i + batch_size, len(X_test_normal_windows))\n",
    "            X_batch = X_test_normal_windows[i:end_idx].to(device)\n",
    "            \n",
    "            reconstruction_errors = detector.autoencoder.get_reconstruction_error(X_batch)\n",
    "            errors.extend(reconstruction_errors.cpu().numpy())\n",
    "            \n",
    "            del X_batch, reconstruction_errors\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set threshold at 95th percentile (or specified percentile)\n",
    "    if errors:\n",
    "        threshold = np.percentile(errors, percentile)\n",
    "        print(f\"Threshold set to {threshold:.4f} based on {percentile}th percentile of normal operation data\")\n",
    "        detector.threshold = threshold\n",
    "    else:\n",
    "        print(\"Warning: Could not compute errors for threshold. Using default.\")\n",
    "        detector.threshold = 0.1\n",
    "    \n",
    "    return detector.threshold\n",
    "\n",
    "def evaluate_all_faults(detector, df_test_faultfree, df_test_faulty, scaler, lookback=15, batch_size=2,\n",
    "                         skip_faults=None, output_dir='results'):\n",
    "    \"\"\"\n",
    "    Evaluate detector on all fault types and generate summary\n",
    "    \"\"\"\n",
    "    if skip_faults is None:\n",
    "        skip_faults = [3, 9, 15]  # Default skipped faults as in original code\n",
    "        \n",
    "    # Define fault types for evaluation: 0 and 1-20 excluding skipped faults\n",
    "    all_dataset_fault_numbers = sorted(df_test_faulty['faultNumber'].unique())\n",
    "    fault_types_to_evaluate = [0]  # Start with fault type 0 (normal)\n",
    "    \n",
    "    for ft in all_dataset_fault_numbers:\n",
    "        if ft not in skip_faults:\n",
    "            fault_types_to_evaluate.append(ft)\n",
    "    \n",
    "    print(f\"Will evaluate for fault types: {fault_types_to_evaluate}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Store results for all fault types\n",
    "    all_results = {}\n",
    "    \n",
    "    # Evaluate each fault type\n",
    "    for fault_type in fault_types_to_evaluate:\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        result = evaluate_fault_detector(\n",
    "            detector=detector,\n",
    "            fault_type=fault_type,\n",
    "            df_test_faultfree=df_test_faultfree, \n",
    "            df_test_faulty=df_test_faulty,    \n",
    "            scaler=scaler,\n",
    "            lookback=lookback,\n",
    "            batch_size=batch_size,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            all_results[fault_type] = result\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for fault_type, result in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Fault Type': fault_type,\n",
    "            'Precision': result.get('precision', 0),\n",
    "            'Recall': result.get('recall', 0),\n",
    "            'F1 Score': result.get('f1', 0),\n",
    "            'Accuracy': result.get('accuracy', 0),\n",
    "            'ROC-AUC': result.get('roc_auc', np.nan),\n",
    "            'PR-AUC': result.get('pr_auc', np.nan),\n",
    "            'TP': result.get('true_positives', 0),\n",
    "            'FP': result.get('false_positives', 0),\n",
    "            'TN': result.get('true_negatives', 0),\n",
    "            'FN': result.get('false_negatives', 0)\n",
    "        })\n",
    "    \n",
    "    # Save summary results to CSV and display\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(f\"{output_dir}/fault_detection_summary.csv\", index=False)\n",
    "    print(\"\\nSummary of results for all evaluated fault types:\")\n",
    "    print(summary_df)\n",
    "    \n",
    "    # Create summary visualizations\n",
    "    if len(summary_data) > 0:\n",
    "        # Bar chart of F1 scores by fault type\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        fault_types = summary_df['Fault Type'].values\n",
    "        f1_scores = summary_df['F1 Score'].values\n",
    "        \n",
    "        # Sort by fault type\n",
    "        sort_idx = np.argsort(fault_types)\n",
    "        fault_types = fault_types[sort_idx]\n",
    "        f1_scores = f1_scores[sort_idx]\n",
    "        \n",
    "        bars = plt.bar(fault_types, f1_scores, color='skyblue')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=9)\n",
    "        \n",
    "        plt.title('F1 Scores by Fault Type', fontsize=16)\n",
    "        plt.xlabel('Fault Type', fontsize=14)\n",
    "        plt.ylabel('F1 Score', fontsize=14)\n",
    "        plt.xticks(fault_types)\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/summary_f1_scores.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create confusion matrix components visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Create subplots for TP, FP, TN, FN\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.bar(fault_types, summary_df['TP'].values[sort_idx], color='green')\n",
    "        plt.title('True Positives by Fault Type')\n",
    "        plt.xlabel('Fault Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(fault_types)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar(fault_types, summary_df['FP'].values[sort_idx], color='red')\n",
    "        plt.title('False Positives by Fault Type')\n",
    "        plt.xlabel('Fault Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(fault_types)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.bar(fault_types, summary_df['TN'].values[sort_idx], color='blue')\n",
    "        plt.title('True Negatives by Fault Type')\n",
    "        plt.xlabel('Fault Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(fault_types)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.bar(fault_types, summary_df['FN'].values[sort_idx], color='orange')\n",
    "        plt.title('False Negatives by Fault Type')\n",
    "        plt.xlabel('Fault Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(fault_types)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/summary_confusion_components.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"\\nAll summary visualizations saved to {output_dir}/\")\n",
    "    return all_results\n",
    "\n",
    "def main_testing():\n",
    "    \"\"\"\n",
    "    Main function for testing - modified to match original implementation\n",
    "    \"\"\"\n",
    "    lookback = 15\n",
    "    batch_size = 2  # Smaller batch size for testing to avoid memory issues\n",
    "    output_dir = 'fault_evaluation_results'\n",
    "    \n",
    "    # Load model and scaler\n",
    "    detector, scaler = load_model_and_scaler()\n",
    "    \n",
    "    if detector is None or scaler is None:\n",
    "        print(\"Error: Could not load model or scaler. Please run training first.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure threshold is set correctly\n",
    "    if detector.threshold is None or detector.threshold < 0.001:\n",
    "        print(\"Threshold not set or invalid. Setting threshold from normal data.\")\n",
    "        set_threshold_from_normal_data(detector, scaler, lookback=lookback, percentile=95, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Using threshold: {detector.threshold}\")\n",
    "    \n",
    "    # Evaluate all fault types\n",
    "    results = evaluate_all_faults(\n",
    "        detector=detector,\n",
    "        df_test_faultfree=df_test_faultfree,\n",
    "        df_test_faulty=df_test_faulty,\n",
    "        scaler=scaler,\n",
    "        lookback=lookback,\n",
    "        batch_size=batch_size,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTesting completed successfully!\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966b404-de1b-42e5-88cc-73879ebb43fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
